#!/usr/bin/env python3

import boto3
import os
import json
import csv
import gzip
from pprint import pprint
import sqlite3
from sqlite3 import Error
import pandas as pd

# Define sessions for security and target security.


def define_session_clients(account):
    session = boto3.Session(profile_name=account)
    sts_client = session.client('sts')
    s3_client = session.client('s3')
    iam_client = session.client('iam')
    ecr_client = session.client('ecr')
    return session, sts_client, s3_client, iam_client, ecr_client

# Step2: Download the logs from the flaws2-logs bucket.

# List the bucket names in the security session.


def list_bucket_names(s3_client):
    buckets = s3_client.list_buckets()
    for bucket in buckets['Buckets']:
        print(bucket['Name'])

# Define the flaws2-logs bucket.


def define_bucket(session, bucket):
    s3_resource = session.resource('s3')
    return s3_resource.Bucket(bucket)

# Function to list files in an s3 bucket


def s3_ls(bucket):
    for bucket_obj in bucket.objects.all():
        print(bucket_obj)

# Function to download files from an s3 bucket


def s3_sync(bucket, target_directory):
    # ToDo (smorton): Replace hardcoded directory with one generated by the script.
    nested_directory = os.path.join(
        target_directory, 'AWSLogs/653711331788/CloudTrail/us-east-1/2018/11/28/')
    if not os.path.exists(nested_directory):
        os.makedirs(nested_directory)
    files = set(os.listdir(nested_directory))
    print(nested_directory)
    for obj in bucket.objects.all():
        obj_filename = obj.key
        if obj_filename not in files:
            bucket.download_file(obj_filename, os.path.join(
                target_directory, obj_filename))

# Step 1: Function to find all of the files in the directory and add them to a list.


def list_all_files(downloaded_directory):
    file_list = []
    for root, directories, files in os.walk(downloaded_directory):
        for file in files:
            if '.gz' in file:
                file_list.append(os.path.join(root, file))
    return file_list

# Step 2: Functions to write the contents of a file to a tsv file.


def write_tsv_rows(open_mode, tsv_path, gz_path):
    with open(tsv_path, open_mode) as output_file, gzip.open(gz_path, 'rt') as json_file:
        json_dict = json.load(json_file)
        for record in json_dict['Records']:
            cleaned_dict = record
            final_dict = {k: cleaned_dict[k] for k in (
                'eventTime', 'sourceIPAddress', 'eventName')}
            identity_dict = {k: cleaned_dict['userIdentity'].get(
                k, None) for k in ('arn', 'accountId', 'type')}
            final_dict.update(identity_dict)
            tsv = csv.DictWriter(
                output_file, final_dict.keys(), delimiter='\t')
            if open_mode == 'w':
                tsv.writeheader()
            tsv.writerow(final_dict)


def logs_to_tsv(tsv_path, gz_path):
    if os.path.isfile(tsv_path):
        write_tsv_rows('a+', tsv_path, gz_path)
    else:
        write_tsv_rows('w', tsv_path, gz_path)

# Step 3: Combine the two.  Find all of the files in the directory and update the tsv file with those logs.


def write_to_tsv(target_tsv, source_directory):
    files = list_all_files(source_directory)
    for file in files:
        logs_to_tsv(target_tsv, file)

# Step 1: Search the first suspicious call.


def find_event(event_name, source_directory):
    files = list_all_files(source_directory)
    for file in files:
        with gzip.open(file, 'rt') as json_file:
            json_dict = json.load(json_file)
            for record in json_dict['Records']:
                cleaned_dict = record
                if cleaned_dict['eventName'] == event_name:
                    pprint(cleaned_dict)

# Step 2: Get details for the suspicious role.


def get_role_details(profile, role):
    iam_client = boto3.Session(profile_name=profile).client('iam')
    iam_role = iam_client.get_role(RoleName=role)
    pprint(iam_role)

# Step 2: Check the policy.


def get_policy_details(profile, repository):
    ecr_client = boto3.Session(profile_name=profile).client('ecr')
    ecr_policy = ecr_client.get_repository_policy(repositoryName=repository)
    pprint(ecr_policy)


def create_log_table(database):
    conn = sqlite3.connect(database)
    c = conn.cursor()
    # Create SQL Table.
    create_table_sql = '''
    CREATE TABLE IF NOT EXISTS cloudtrail (
        event_id TEXT PRIMARY KEY,
        event_version TEXT,
        event_type TEXT,
        event_time TEXT,
        event_source TEXT,
        event_name TEXT,
        user_identity TEXT,
        aws_region TEXT,
        source_ip_address TEXT,
        user_agent TEXT,
        error_code TEXT,
        error_message TEXT,
        request_parameters TEXT,
        response_elements TEXT,
        additional_event_data TEXT,
        request_id TEXT,
        resources TEXT,
        api_version TEXT,
        read_only TEXT,
        recipient_account_id TEXT,
        service_event_details TEXT,
        shared_event_id,
        vpc_endpoint_id TEXT
    );'''
    c.execute(create_table_sql)
    conn.commit()
    c.close()


sql_columns = ['eventID', 'eventVersion', 'eventType', 'eventTime', 'eventSource', 'eventName',
               'userIdentity', 'awsRegion', 'sourceIPAddress', 'userAgent', 'errorCode',
               'errorMessage', 'requestParameters', 'responseElements', 'additionalEventData',
               'requestId', 'resources', 'apiVersion', 'readOnly', 'recipientAccountID',
               'serviceEventDetails', 'sharedEventID', 'vpcEndpointID']

# Insert data into SQL table.


def write_sql_table(gz_path):
    json_file = gzip.open(gz_path, 'rt')
    json_dict = json.load(json_file)
    query = 'INSERT INTO cloudtrail VALUES(' + \
        ','.join('?' * len(sql_columns)) + ');'
    record = json_dict['Records']
    for record in json_dict['Records']:
        # Prep record for insert.
        values = tuple(record.get(c, None) for c in sql_columns)
        str_values = tuple(map(str, values))
        # Connect to database and insert record.
        db = sqlite3.connect('logs')
        c = db.cursor()
        c.execute(query, str_values)
        db.commit()

# write_sql_table('test/AWSLogs/653711331788/CloudTrail/us-east-1/2018/11/28/653711331788_CloudTrail_us-east-1_20181128T2310Z_A1lhv3sWzzRIBFVk.json.gz')


def main():
    # Objective 1: Download CloudTrail logs

    # Define security sessions and clients.
    security_session, security_sts_client, security_s3_client, security_iam_client, security_ecr_client = define_session_clients(
        'security')

    # Get the security client's caller identity.
    print(security_sts_client.get_caller_identity())

    # List the bucket names in the security_session.
    list_bucket_names(security_s3_client)

    # Define the flaws2-logs bucket.
    flaws2_logs_bucket = define_bucket(security_session, 'flaws2-logs')

    # List files in flaws2-logs
    s3_ls(flaws2_logs_bucket)

    # Download the files.
    s3_sync(flaws2_logs_bucket, 'test')

    # Objective 2: Access the target account.

    # Define target_security and list its buckets.
    target_session, target_sts_client, target_s3_client, target_iam_client, target_ecr_client = define_session_clients(
        'target_security')
    list_bucket_names(target_s3_client)
    flaws2cloud_bucket = define_bucket(target_session, 'flaws2.cloud')

    # Define sts client for target_security and get the target_security caller identity.
    print(target_sts_client.get_caller_identity())

    # Objective 3: Read Log Data

    # Create tsv file with the target logs.
    write_to_tsv('target_logs.tsv', 'test')

    # Objective 4: Identify credential theft.

    # Print the json object for the event that we're interested in.
    find_event('ListBuckets', 'test')

    # Print role details for level3
    get_role_details('target_security', 'level3')

    # Objective 5: Identify the public resource.

    # View the ListImages call event.
    find_event('ListImages', 'test')

    # Print the policy details.
    get_policy_details('target_security', 'level2')

    # Objective 6: Query the logs.

    create_log_table('logs')

    sql_files = list_all_files('test')
    for file in sql_files:
        write_sql_table(file)


if __name__ == "__main__":
    main()
