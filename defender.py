#!/usr/bin/env python3

"""Flaws2 Defender

This script walks the user through the steps from the Flaws2 Defender track at flaws2.cloud.
The script assumes that you have set up access to an AWS account through the command line.

The script requires the following packages be installed in the Python environment you are working in:
  - boto3
  - pandas
"""

import boto3
import os
import json
import csv
import gzip
from pprint import pprint
import sqlite3
from sqlite3 import Error
import pandas as pd

# Define sessions for security and target security.


def define_session_clients(account):
    """ Defines the AWS session and relevant clients for a profile.

    Args:
        account (str): The AWS profile to use.  Either the security profile or the target profile.

    Returns:
        list:  A list containing the session definition and STS, s3, IAM, and ECR client objects.

    Examples:
        >>> target_session, target_sts_client, target_s3_client, target_iam_client, target_ecr_client = define_session_clients('target_security')
    """
    session = boto3.Session(profile_name=account)
    sts_client = session.client('sts')
    s3_client = session.client('s3')
    iam_client = session.client('iam')
    ecr_client = session.client('ecr')
    return session, sts_client, s3_client, iam_client, ecr_client

# Step2: Download the logs from the flaws2-logs bucket.

# List the bucket names in the security session.


def list_bucket_names(s3_client):
    """ Prints the names of the s3 buckets owned by an AWS profile. """
    buckets = s3_client.list_buckets()
    for bucket in buckets['Buckets']:
        print(bucket['Name'])

# Define the flaws2-logs bucket.


def define_bucket(session, bucket):
    """ Returns an s3 bucket object for a session and bucket. """ 
    s3_resource = session.resource('s3')
    return s3_resource.Bucket(bucket)

# Function to list files in an s3 bucket


def s3_ls(bucket):
    """ Prints the ObjectSummary of each object in an s3 bucket. """
    for bucket_obj in bucket.objects.all():
        print(bucket_obj)

# Function to download files from an s3 bucket


def s3_sync(bucket, target_directory):
    """ Downloads all of the files in an s3 bucket to a local sub-directory.

    Args:
        bucket (str): The name of the s3 bucket to download.
        target_directory (str): The relative path to the directory to download files to.

    Returns:
        None
    """
    # ToDo (smorton): Replace hardcoded directory with one generated by the script.
    nested_directory = os.path.join(
        target_directory, 'AWSLogs/653711331788/CloudTrail/us-east-1/2018/11/28/')
    if not os.path.exists(nested_directory):
        os.makedirs(nested_directory)
    files = set(os.listdir(nested_directory))
    print(nested_directory)
    for obj in bucket.objects.all():
        obj_filename = obj.key
        if obj_filename not in files:
            bucket.download_file(obj_filename, os.path.join(
                target_directory, obj_filename))

# Step 1: Function to find all of the files in the directory and add them to a list.


def list_all_files(downloaded_directory):
    """ Returns a list of the gzip files in a directory.
    
    Args:
        downloaded_directory (str): The name of the directory to search for gzip files.

    Returns:
        list: A list of the files in the directory.
    """
    file_list = []
    for root, directories, files in os.walk(downloaded_directory):
        for file in files:
            if '.gz' in file:
                file_list.append(os.path.join(root, file))
    return file_list

# Step 2: Functions to write the contents of a file to a tsv file.


def write_tsv_rows(open_mode, tsv_path, gz_path):
    """ Writes the contents of a gzip file to a .tsv formatted text file.

    Args:
        open_mode (str): The mode to use when opening the .tsv file.
        tsv_path (str): The relative path to write the tsv file to.
        gz_path (str): The relative path to the gz file that will have its contents copied.

    Returns:
        None
    """
    with open(tsv_path, open_mode) as output_file, gzip.open(gz_path, 'rt') as json_file:
        json_dict = json.load(json_file)
        for record in json_dict['Records']:
            cleaned_dict = record
            final_dict = {k: cleaned_dict[k] for k in (
                'eventTime', 'sourceIPAddress', 'eventName')}
            identity_dict = {k: cleaned_dict['userIdentity'].get(
                k, None) for k in ('arn', 'accountId', 'type')}
            final_dict.update(identity_dict)
            tsv = csv.DictWriter(
                output_file, final_dict.keys(), delimiter='\t')
            if open_mode == 'w':
                tsv.writeheader()
            tsv.writerow(final_dict)


def logs_to_tsv(tsv_path, gz_path):
    """ Determines the open mode to use when running write_tsv_rows(). """
    if os.path.isfile(tsv_path):
        write_tsv_rows('a+', tsv_path, gz_path)
    else:
        write_tsv_rows('w', tsv_path, gz_path)

# Step 3: Combine the two.  Find all of the files in the directory and update the tsv file with those logs.


def write_to_tsv(target_tsv, source_directory):
    """ Finds all of the gzip files in a directory and writes their contents to a tsv file. """
    files = list_all_files(source_directory)
    for file in files:
        logs_to_tsv(target_tsv, file)

# Step 1: Search the first suspicious call.


def find_event(event_name, source_directory):
    """ Searches all gzip files in a directory for an event record.  Prints the record when found.

    Args:
        event_name (str): The name of the event to search for.
        source_directory (str): The name of the directory to scan for the event.

    Returns:
        None
    """
    files = list_all_files(source_directory)
    for file in files:
        with gzip.open(file, 'rt') as json_file:
            json_dict = json.load(json_file)
            for record in json_dict['Records']:
                cleaned_dict = record
                if cleaned_dict['eventName'] == event_name:
                    pprint(cleaned_dict)

# Step 2: Get details for the suspicious role.


def get_role_details(profile, role):
    """ Prints information about a profile's IAM role. """
    iam_client = boto3.Session(profile_name=profile).client('iam')
    iam_role = iam_client.get_role(RoleName=role)
    pprint(iam_role)

# Step 2: Check the policy.


def get_policy_details(profile, repository):
    """ Prints information about the repository policy for a given repository. """
    ecr_client = boto3.Session(profile_name=profile).client('ecr')
    ecr_policy = ecr_client.get_repository_policy(repositoryName=repository)
    pprint(ecr_policy)


def create_log_table(database):
    conn = sqlite3.connect(database)
    c = conn.cursor()
    # Create SQL Table.
    create_table_sql = '''
    CREATE TABLE IF NOT EXISTS cloudtrail (
        event_id TEXT PRIMARY KEY,
        event_version TEXT,
        event_type TEXT,
        event_time TEXT,
        event_source TEXT,
        event_name TEXT,
        user_identity TEXT,
        aws_region TEXT,
        source_ip_address TEXT,
        user_agent TEXT,
        error_code TEXT,
        error_message TEXT,
        request_parameters TEXT,
        response_elements TEXT,
        additional_event_data TEXT,
        request_id TEXT,
        resources TEXT,
        api_version TEXT,
        read_only TEXT,
        recipient_account_id TEXT,
        service_event_details TEXT,
        shared_event_id,
        vpc_endpoint_id TEXT
    );'''
    c.execute(create_table_sql)
    conn.commit()
    c.close()


sql_columns = ['eventID', 'eventVersion', 'eventType', 'eventTime', 'eventSource', 'eventName',
               'userIdentity', 'awsRegion', 'sourceIPAddress', 'userAgent', 'errorCode',
               'errorMessage', 'requestParameters', 'responseElements', 'additionalEventData',
               'requestId', 'resources', 'apiVersion', 'readOnly', 'recipientAccountID',
               'serviceEventDetails', 'sharedEventID', 'vpcEndpointID']

# Insert data into SQL table.


def write_sql_table(gz_path):
    json_file = gzip.open(gz_path, 'rt')
    json_dict = json.load(json_file)
    query = 'INSERT INTO cloudtrail VALUES(' + \
        ','.join('?' * len(sql_columns)) + ');'
    record = json_dict['Records']
    for record in json_dict['Records']:
        # Prep record for insert.
        values = tuple(record.get(c, None) for c in sql_columns)
        str_values = tuple(map(str, values))
        # Connect to database and insert record.
        db = sqlite3.connect('logs')
        c = db.cursor()
        c.execute(query, str_values)
        db.commit()

# write_sql_table('test/AWSLogs/653711331788/CloudTrail/us-east-1/2018/11/28/653711331788_CloudTrail_us-east-1_20181128T2310Z_A1lhv3sWzzRIBFVk.json.gz')


def main():
    # Objective 1: Download CloudTrail logs

    # Define security sessions and clients.
    security_session, security_sts_client, security_s3_client, security_iam_client, security_ecr_client = define_session_clients(
        'security')

    # Get the security client's caller identity.
    print(security_sts_client.get_caller_identity())

    # List the bucket names in the security_session.
    list_bucket_names(security_s3_client)

    # Define the flaws2-logs bucket.
    flaws2_logs_bucket = define_bucket(security_session, 'flaws2-logs')

    # List files in flaws2-logs
    s3_ls(flaws2_logs_bucket)

    # Download the files.
    s3_sync(flaws2_logs_bucket, 'test')

    # Objective 2: Access the target account.

    # Define target_security and list its buckets.
    target_session, target_sts_client, target_s3_client, target_iam_client, target_ecr_client = define_session_clients(
        'target_security')
    list_bucket_names(target_s3_client)
    flaws2cloud_bucket = define_bucket(target_session, 'flaws2.cloud')

    # Define sts client for target_security and get the target_security caller identity.
    print(target_sts_client.get_caller_identity())

    # Objective 3: Read Log Data

    # Create tsv file with the target logs.
    write_to_tsv('target_logs.tsv', 'test')

    # Objective 4: Identify credential theft.

    # Print the json object for the event that we're interested in.
    find_event('ListBuckets', 'test')

    # Print role details for level3
    get_role_details('target_security', 'level3')

    # Objective 5: Identify the public resource.

    # View the ListImages call event.
    find_event('ListImages', 'test')

    # Print the policy details.
    get_policy_details('target_security', 'level2')

    # Objective 6: Query the logs.

    create_log_table('logs')

    sql_files = list_all_files('test')
    for file in sql_files:
        write_sql_table(file)


if __name__ == "__main__":
    main()
